#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
gen_concat_tests.py

Generate pytest test suites for `pandas.concat` (or any extracted function)
using one or more LLMs via either:
    - Ollama (local models)
    - OpenRouter (cloud models)

This script:
    • Loads a JSON spec generated by fetch.py (recommended)
      OR a raw Python source file.
    • Builds a rich prompt including:
        – exact function source
        – signature
        – examples
        – pandas version
        – environment metadata
    • Sends the prompt to multiple models (OpenRouter/Ollama)
    • Saves each model’s generated test suite as a .py file

Typical usage:
    python gen_concat_tests.py \
        --src pandas_concat.json \
        --out-pattern "generated/{model}.py" \
        --models "mistral-7b-instruct:free,qwen3-coder:free" \
        --default-provider openrouter

The test generation stage is fully automated and designed to work with ANY
Python function extracted via fetch.py—not just pandas.concat.
"""

from __future__ import annotations

import argparse
import json
import os
import re
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Iterator, Optional, Sequence, Tuple

import requests
import pandas as pd


# Global Configuration Defaults

DEFAULT_OLLAMA_HOST = "http://127.0.0.1:11434"
DEFAULT_MODEL = "llama3:8b-instruct"
DEFAULT_TEMPERATURE = 0.2
DEFAULT_TIMEOUT = 600
DEFAULT_HEALTH_TIMEOUT = 5
DEFAULT_RETRIES = 2
DEFAULT_BACKOFF = 2.0
DEFAULT_NUM_CTX = 8192

# OpenRouter defaults
DEFAULT_OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1/chat/completions"
DEFAULT_OPENROUTER_TIMEOUT = 600
ENV_OPENROUTER_API_KEY = "OPENROUTER_API_KEY"

# Optional hardcoded API key for convenience during local testing.
HARDCODED_OPENROUTER_API_KEY: Optional[str] = None


# Prompt Template (LLM Instruction)

PROMPT_TEMPLATE = """
You are a precise code generator and a senior Python test engineer.
Output ONLY valid Python code (no explanations, no markdown fences).

Target environment:
- Python: {python_version}
- pandas: {pandas_version}

Task:
Given the exact source of the function `pandas.concat` (below),
write a comprehensive `pytest` test file named "{outfile}" that validates
its behavior across edge cases.

Important constraints:
- All tests MUST pass on the given pandas version.
- Avoid undocumented or private API behavior.
- Use only stable and documented parameters supported by this version.
- Tests must be deterministic, no random data, no I/O, no network.

Coverage requirements:
- axis (0/1)
- keys / levels / names
- ignore_index
- join = ('outer', 'inner')
- sort
- verify_integrity
- Series vs DataFrame concatenation
- empty inputs and mixed dtypes
- non-unique index behavior
- MultiIndex scenarios
- datetime/categorical/bool/object/numeric dtype mixes
- handling of views/copies if applicable

Use:
- pytest
- pandas.testing.assert_frame_equal
- pandas.testing.assert_series_equal

Prefer:
- Many small focused tests
- Clear naming (test_*)
- Self-contained and minimal imports

Function signature (for reference):
{signature_block}

Selected examples from documentation:
{examples_block}

Function source:
<<<BEGIN_SOURCE
{func_source}
<<<END_SOURCE
"""


# Basic Utilities

def read_text(path: Path) -> str:
    """Read a UTF-8 file and return its contents."""
    if not path.exists():
        raise FileNotFoundError(f"Source file not found: {path}")
    return path.read_text(encoding="utf-8")


def extract_code_block(text: str) -> str:
    """
    If the LLM output contains fenced code (```python ... ```), extract the inner code.
    Otherwise return the raw text.
    """
    m = re.search(r"```(?:python)?\s*(.*?)```", text, flags=re.S)
    return m.group(1).strip() if m else text.strip()


def sanitize_filename_piece(s: str) -> str:
    """Sanitize strings for safe filesystem usage."""
    return re.sub(r"[^A-Za-z0-9_.-]+", "-", s)


def load_source_and_spec(src: Path) -> Tuple[str, Optional[dict]]:
    """
    Load a function specification:
        • If src is JSON → return (body_stripped, full spec)
        • If src is .py or raw → return (content, None)
    """
    text = read_text(src)
    if src.suffix.lower() == ".json":
        spec = json.loads(text)
        body = spec.get("body_stripped", "")
        if not body:
            raise RuntimeError("JSON spec missing 'body_stripped'.")
        return body, spec
    return text, None


# Prompt Construction

def build_prompt(func_source: str, outfile: str, spec: Optional[dict]) -> str:
    """Construct the full LLM prompt with environment + signature + examples."""
    python_version = ".".join(map(str, sys.version_info[:3]))

    if spec is not None:
        pandas_version = spec.get("version", pd.__version__)
        signature = spec.get("signature", "")
        examples = spec.get("examples_code") or []
    else:
        pandas_version = pd.__version__
        signature = ""
        examples = []

    signature_block = signature or "(signature not provided in spec)"

    if examples:
        examples_block = "\n\n".join(
            f"Example {i+1}:\n{code}" for i, code in enumerate(examples[:3])
        )
    else:
        examples_block = "(no examples provided in spec)"

    return PROMPT_TEMPLATE.format(
        func_source=func_source,
        outfile=outfile,
        python_version=python_version,
        pandas_version=pandas_version,
        signature_block=signature_block,
        examples_block=examples_block,
    )


# OpenRouter Model Aliases (normalize user-friendly model names)

OPENROUTER_MODEL_ALIASES = {
    "deepseek-r1t2-chimera:free": "tngtech/deepseek-r1t2-chimera:free",
    "qwen3-coder:free": "qwen/qwen3-coder:free",
    "gemma-3-4b-it:free": "google/gemma-3-4b-it:free",
    "llama-3.3-70b-instruct:free": "meta-llama/llama-3.3-70b-instruct:free",
    "sherlock-dash-alpha": "openrouter/sherlock-dash-alpha",
    "mistral-7b-instruct:free": "mistralai/mistral-7b-instruct:free",
}


def normalize_openrouter_model(model: str) -> str:
    """
    Convert lightweight friendly model IDs into full OpenRouter slugs.
    If model already contains '/', return it as-is.
    """
    if "/" in model:
        return model
    return OPENROUTER_MODEL_ALIASES.get(model, model)


# Ollama Client Implementation

@dataclass
class OllamaOptions:
    host: str = DEFAULT_OLLAMA_HOST
    model: str = DEFAULT_MODEL
    temperature: float = DEFAULT_TEMPERATURE
    timeout: int = DEFAULT_TIMEOUT
    health_timeout: int = DEFAULT_HEALTH_TIMEOUT
    retries: int = DEFAULT_RETRIES
    backoff: float = DEFAULT_BACKOFF
    stream: bool = True
    num_ctx: int = DEFAULT_NUM_CTX


class OllamaClient:
    """Minimal wrapper around the Ollama API endpoints."""

    def __init__(self, opts: OllamaOptions) -> None:
        self.opts = opts

    # Health Checks ------------------------------------------------------------

    def health(self) -> None:
        """Ping Ollama server to verify readiness."""
        url = f"{self.opts.host}/api/tags"
        r = requests.get(url, timeout=self.opts.health_timeout)
        r.raise_for_status()

    def model_available(self, model: str | None = None) -> bool:
        """Check if the given model has been pulled into Ollama."""
        model = model or self.opts.model
        url = f"{self.opts.host}/api/tags"
        r = requests.get(url, timeout=self.opts.health_timeout)
        r.raise_for_status()
        for m in r.json().get("models", []):
            if m.get("model") == model or m.get("name") == model:
                return True
        return False

    # Generation ---------------------------------------------------------------

    def _payload(self, prompt: str, stream: bool) -> dict:
        """Assemble JSON body for Ollama API."""
        return {
            "model": self.opts.model,
            "prompt": prompt,
            "temperature": self.opts.temperature,
            "stream": stream,
            "options": {"num_ctx": self.opts.num_ctx},
        }

    def generate_stream(self, prompt: str) -> Iterator[str]:
        """Stream incremental output from Ollama."""
        url = f"{self.opts.host}/api/generate"
        payload = self._payload(prompt, stream=True)
        with requests.post(url, json=payload, stream=True, timeout=self.opts.timeout) as r:
            r.raise_for_status()
            for line in r.iter_lines(decode_unicode=True):
                if line:
                    try:
                        obj = json.loads(line)
                        chunk = obj.get("response", "")
                        if chunk:
                            yield chunk
                    except json.JSONDecodeError:
                        continue

    def generate_once(self, prompt: str) -> str:
        """Request full completion in a single non-stream call."""
        url = f"{self.opts.host}/api/generate"
        payload = self._payload(prompt, stream=False)
        r = requests.post(url, json=payload, timeout=self.opts.timeout)
        r.raise_for_status()
        return r.json().get("response", "")

    def generate(self, prompt: str) -> str:
        """
        Execute a robust generation with retry handling.
        Uses streaming if enabled; falls back to non-stream if needed.
        """
        last_err: Optional[Exception] = None

        for attempt in range(1, self.opts.retries + 2):
            try:
                self.health()

                if self.opts.stream:
                    chunks = [chunk for chunk in self.generate_stream(prompt)]
                    text = "".join(chunks)
                    if text.strip():
                        return text

                text = self.generate_once(prompt)
                if text.strip():
                    return text

                raise RuntimeError("Ollama returned an empty response.")

            except Exception as e:
                last_err = e
                if attempt <= self.opts.retries:
                    time.sleep(self.opts.backoff * attempt)
                else:
                    raise RuntimeError(f"Ollama request failed after retries: {last_err}")

        return ""  # unreachable safeguard


# OpenRouter Client Implementation

@dataclass
class OpenRouterOptions:
    base_url: str = DEFAULT_OPENROUTER_BASE_URL
    api_key: Optional[str] = None
    model: str = ""
    temperature: float = DEFAULT_TEMPERATURE
    timeout: int = DEFAULT_OPENROUTER_TIMEOUT
    retries: int = DEFAULT_RETRIES
    backoff: float = DEFAULT_BACKOFF
    site_url: Optional[str] = None
    app_name: Optional[str] = "gen-concat-tests"


class OpenRouterClient:
    """HTTP client for the OpenRouter Conversational Completion API."""

    def __init__(self, opts: OpenRouterOptions) -> None:
        self.opts = opts
        self.session = requests.Session()

    def _resolve_api_key(self) -> str:
        """Determine which API key should be used."""
        if HARDCODED_OPENROUTER_API_KEY:
            return HARDCODED_OPENROUTER_API_KEY
        if self.opts.api_key:
            return self.opts.api_key
        if env := os.getenv(ENV_OPENROUTER_API_KEY):
            return env
        raise RuntimeError("Missing OpenRouter API key.")

    def generate(self, prompt: str) -> str:
        """
        Perform a chat completion request to OpenRouter.
        Includes retry logic and explicit error surfacing.
        """
        api_key = self._resolve_api_key()

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }
        if self.opts.site_url:
            headers["HTTP-Referer"] = self.opts.site_url
        if self.opts.app_name:
            headers["X-Title"] = self.opts.app_name

        payload = {
            "model": self.opts.model,
            "temperature": self.opts.temperature,
            "messages": [{"role": "user", "content": prompt}],
        }

        last_err: Optional[Exception] = None

        for attempt in range(1, self.opts.retries + 2):
            try:
                r = self.session.post(
                    self.opts.base_url,
                    headers=headers,
                    json=payload,
                    timeout=self.opts.timeout,
                )

                if r.status_code >= 400:
                    try:
                        details = r.json()
                    except Exception:
                        details = {"error_text": r.text}
                    r.raise_for_status()

                data = r.json()

                if not data.get("choices"):
                    raise RuntimeError("OpenRouter returned no completions.")

                content = data["choices"][0]["message"]["content"]
                if isinstance(content, str):
                    return content.strip()

                if isinstance(content, list):
                    return "".join(
                        (item.get("text", "") if isinstance(item, dict) else str(item))
                        for item in content
                    ).strip()

                return str(content).strip()

            except Exception as e:
                last_err = e
                if attempt <= self.opts.retries:
                    time.sleep(self.opts.backoff * attempt)
                else:
                    raise RuntimeError(f"OpenRouter error after retries: {last_err}")

        return ""


# Core App Logic: Running Models & Writing Outputs

def parse_models_arg(models_str: str, default_provider: str) -> list[Tuple[str, str]]:
    """
    Convert the command-line --models string into structured pairs:
        [
            ("openrouter", "google/gemma-3-27b-it:free"),
            ("ollama", "llama3:8b-instruct"),
        ]
    """
    known_prefixes = ("openrouter:", "ollama:")
    items = []

    for raw in (s.strip() for s in models_str.split(",") if s.strip()):
        lower = raw.lower()
        if lower.startswith(known_prefixes):
            first_colon = raw.find(":")
            provider = raw[:first_colon].lower()
            model = raw[first_colon + 1:].strip()
        else:
            provider = default_provider.lower()
            model = raw
        items.append((provider, model))

    return items


def run_one(
    provider: str,
    model: str,
    prompt: str,
    *,
    ollama_opts: OllamaOptions,
    openrouter_opts: OpenRouterOptions,
) -> str:
    """
    Dispatch a prompt to either an Ollama or OpenRouter backend.
    """
    if provider == "ollama":
        client = OllamaClient(OllamaOptions(**{**ollama_opts.__dict__, "model": model}))
        try:
            if not client.model_available(model):
                print(f"[WARN] Ollama model '{model}' not found (may require 'ollama pull {model}').",
                      file=sys.stderr)
        except Exception:
            pass
        return client.generate(prompt)

    if provider == "openrouter":
        normalized = normalize_openrouter_model(model)
        opts = OpenRouterOptions(**{**openrouter_opts.__dict__, "model": normalized})
        return OpenRouterClient(opts).generate(prompt)

    raise ValueError(f"Unknown provider: {provider}")


def run_many(
    src: Path,
    out_pattern: str,
    models: Sequence[Tuple[str, str]],
    *,
    print_prompt: bool,
    ollama_opts: OllamaOptions,
    openrouter_opts: OpenRouterOptions,
) -> int:
    """
    Execute test generation for all selected LLM models.
    """
    func_source, spec = load_source_and_spec(src)

    for provider, model in models:
        safe_model = sanitize_filename_piece(model)
        safe_provider = sanitize_filename_piece(provider)
        out_path = Path(out_pattern.format(provider=safe_provider, model=safe_model))

        prompt = build_prompt(func_source, out_path.name, spec)

        if print_prompt:
            print(f"\n==== Prompt for {provider}:{model} ====\n")
            print(prompt)
            continue

        print(f"→ Running {provider}:{model}")

        try:
            raw = run_one(
                provider,
                model,
                prompt,
                ollama_opts=ollama_opts,
                openrouter_opts=openrouter_opts,
            )

            code = extract_code_block(raw)
            if not code:
                print(f"[ERROR] {provider}:{model} returned empty code.", file=sys.stderr)
                continue

            out_path.parent.mkdir(parents=True, exist_ok=True)
            out_path.write_text(code, encoding="utf-8")

            print(f"[OK] Wrote: {out_path.resolve()}")

        except Exception as e:
            print(f"[FAIL] {provider}:{model} → {e}", file=sys.stderr)

    return 0


# CLI Interface

def parse_args(argv: list[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Generate pytest tests for a function (e.g., pandas.concat) using multiple LLM models."
    )

    parser.add_argument(
        "--src",
        default="pandas_concat.json",
        help="Path to JSON spec (from fetch.py) or raw code file.",
    )
    parser.add_argument(
        "--out-pattern",
        default="generated/{model}.py",
        help="Output filename pattern. Supports {provider} and {model}.",
    )
    parser.add_argument(
        "--models",
        required=True,
        help=(
            "Comma-separated list of LLM models.\n"
            "Use prefix 'openrouter:' or 'ollama:' to force provider.\n"
            "Examples:\n"
            "  openrouter:google/gemma-3-27b-it:free\n"
            "  ollama:llama3:8b-instruct\n"
            "  llama-3.3-70b-instruct:free (with --default-provider openrouter)"
        ),
    )
    parser.add_argument(
        "--default-provider",
        default="openrouter",
        choices=["openrouter", "ollama"],
        help="Provider to assume when no prefix is provided.",
    )

    # Shared generation parameters
    parser.add_argument("--temperature", type=float, default=DEFAULT_TEMPERATURE)

    # Ollama options
    parser.add_argument("--host", default=DEFAULT_OLLAMA_HOST)
    parser.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT)
    parser.add_argument("--health-timeout", type=int, default=DEFAULT_HEALTH_TIMEOUT)
    parser.add_argument("--retries", type=int, default=DEFAULT_RETRIES)
    parser.add_argument("--backoff", type=float, default=DEFAULT_BACKOFF)
    parser.add_argument("--num-ctx", type=int, default=DEFAULT_NUM_CTX)
    parser.add_argument("--no-stream", dest="stream", action="store_false")
    parser.set_defaults(stream=True)

    # OpenRouter options
    parser.add_argument("--openrouter-base-url", default=DEFAULT_OPENROUTER_BASE_URL)
    parser.add_argument("--openrouter-api-key", default=None)
    parser.add_argument("--openrouter-timeout", type=int, default=DEFAULT_OPENROUTER_TIMEOUT)
    parser.add_argument("--openrouter-site-url", default=None)
    parser.add_argument("--openrouter-app-name", default="gen-concat-tests")

    parser.add_argument("--print-prompt", action="store_true")

    return parser.parse_args(argv)


def main(argv: list[str] | None = None) -> int:
    args = parse_args(argv)

    models = parse_models_arg(args.models, default_provider=args.default_provider)

    ollama_opts = OllamaOptions(
        host=args.host,
        model="unused-here",
        temperature=args.temperature,
        timeout=args.timeout,
        health_timeout=args.health_timeout,
        retries=args.retries,
        backoff=args.backoff,
        stream=args.stream,
        num_ctx=args.num_ctx,
    )
    openrouter_opts = OpenRouterOptions(
        base_url=args.openrouter_base_url,
        api_key=args.openrouter_api_key,
        model="unused-here",
        temperature=args.temperature,
        timeout=args.openrouter_timeout,
        retries=args.retries,
        backoff=args.backoff,
        site_url=args.openrouter_site_url,
        app_name=args.openrouter_app_name,
    )

    try:
        return run_many(
            src=Path(args.src),
            out_pattern=args.out_pattern,
            models=models,
            print_prompt=args.print_prompt,
            ollama_opts=ollama_opts,
            openrouter_opts=openrouter_opts,
        )
    except FileNotFoundError as e:
        print(f"ERROR: {e}", file=sys.stderr)
        return 1
    except requests.HTTPError as e:
        print(f"HTTP ERROR: {e}", file=sys.stderr)
        return 3
    except requests.ConnectionError as e:
        print(f"CONNECTION ERROR: {e}", file=sys.stderr)
        return 4
    except Exception as e:
        print(f"UNEXPECTED ERROR: {e}", file=sys.stderr)
        return 5


if __name__ == "__main__":
    raise SystemExit(main())